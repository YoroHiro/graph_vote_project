# votes_enrichis.py

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, broadcast
from datetime import datetime
from pathlib import Path

# -----------------------------
# 1ï¸âƒ£ Chemins des datasets usage
# -----------------------------
VOTES_USAGE_PARQUET = Path("../data_lake/usage/votes")
DEPUTES_USAGE_PARQUET = Path("../data_lake/usage/depute")
ORGANES_USAGE_PARQUET = Path("../data_lake/usage/organes")

OUT_PARQUET = Path("../data_lake/usage/votes_enrichis")

# Dossier datÃ© : AAAAMMJJ
date_str = datetime.now().strftime("%Y%m%d")
CSV_DIR = Path(f"../data_lake/usage/votes_enrichis_csv/{date_str}")
CSV_DIR.mkdir(parents=True, exist_ok=True)

# -----------------------------
# 2ï¸âƒ£ SparkSession
# -----------------------------
spark = (
    SparkSession.builder
    .appName("VotesEnrichis")
    .master("local[*]")
    .getOrCreate()
)

# -----------------------------
# 3ï¸âƒ£ Lecture des donnÃ©es usage
# -----------------------------
print("ğŸ“¥ Lecture des votes (usage)...")
votes = spark.read.parquet(str(VOTES_USAGE_PARQUET))

print("ğŸ“¥ Lecture des dÃ©putÃ©s (usage)...")
deputes = (
    spark.read.parquet(str(DEPUTES_USAGE_PARQUET))
    .withColumnRenamed("uid", "depute_uid")
)

print("ğŸ“¥ Lecture des organes (usage)...")
organes = (
    spark.read.parquet(str(ORGANES_USAGE_PARQUET))
    .withColumnRenamed("uid", "organe_uid")
)

print("ğŸ“„ SchÃ©ma votes :")
votes.printSchema()
print("ğŸ“„ SchÃ©ma deputes :")
deputes.printSchema()
print("ğŸ“„ SchÃ©ma organes :")
organes.printSchema()

# -----------------------------
# 4ï¸âƒ£ Jointure votes âŸ· dÃ©putÃ©s
# -----------------------------
print("ğŸ”— Jointure votes âŸ· dÃ©putÃ©s (broadcast)...")
votes_dep = (
    votes
    .join(
        broadcast(deputes),
        votes.acteurRef == deputes.depute_uid,
        "left"
    )
)

# -----------------------------
# 5ï¸âƒ£ Jointure avec organes (groupe parlementaire)
# -----------------------------
print("ğŸ”— Jointure votes_dep âŸ· organes (groupe)...")
votes_enrichis = (
    votes_dep
    .join(
        broadcast(organes),
        votes_dep.organeRef_groupe == organes.organe_uid,
        "left"
    )
)

# -----------------------------
# 6ï¸âƒ£ SÃ©lection des colonnes finales
# -----------------------------
votes_enrichis = votes_enrichis.select(
    # Colonnes de base du vote
    col("uid_scrutin"),
    col("dateScrutin"),
    col("titre"),
    col("organeRef_groupe"),
    col("acteurRef"),
    col("mandatRef"),
    col("vote_position"),
    col("parDelegation"),
    col("numPlace"),

    # Infos dÃ©putÃ© (prÃ©fixe depute_)
    col("prenom").alias("depute_prenom"),
    col("nom").alias("depute_nom"),
    col("profession").alias("depute_profession"),
    col("catSocPro").alias("depute_catSocPro"),
    col("uri_hatvp").alias("depute_uri_hatvp"),
    col("id_gp").alias("depute_id_gp"),
    col("id_par_pol").alias("depute_id_par_pol"),
    col("nb_mandats").alias("depute_nb_mandats"),

    # Infos organe (prÃ©fixe organe_)
    col("codeType").alias("organe_codeType"),
    col("libelle").alias("organe_libelle"),
    col("libelleAbrev").alias("organe_libelleAbrev"),
)

print("ğŸ“„ SchÃ©ma votes_enrichis :")
votes_enrichis.printSchema()

print("ğŸ” AperÃ§u des premiÃ¨res lignes de votes_enrichis :")
votes_enrichis.show(20, truncate=False)

# -----------------------------
# 7ï¸âƒ£ Sauvegarde en Parquet
# -----------------------------
OUT_PARQUET.parent.mkdir(parents=True, exist_ok=True)

(
    votes_enrichis
    .write
    .mode("overwrite")
    .parquet(str(OUT_PARQUET))
)

print(f"ğŸ’¾ votes_enrichis sauvegardÃ© dans : {OUT_PARQUET}")

# -----------------------------
# 8ï¸âƒ£ Export CSV datÃ© (diagnostic)
# -----------------------------
from datetime import datetime

# Dossier datÃ© au format AAAAMMJJ
date_str = datetime.now().strftime("%Y%m%d")
CSV_DIR = Path(f"../data_lake/usage/votes_enrichis_csv/{date_str}")
CSV_DIR.mkdir(parents=True, exist_ok=True)

# a) Sample CSV
print("ğŸ“¤ Export dâ€™un Ã©chantillon CSV (5000 lignes)...")
(
    votes_enrichis
    .limit(5000)
    .coalesce(1)
    .write
    .option("header", "true")
    .mode("overwrite")
    .csv(str(CSV_DIR / "sample"))
)

# b) Export complet CSV
print("ğŸ“¤ Export CSV complet (peut Ãªtre lourd)...")
(
    votes_enrichis
    .coalesce(1)
    .write
    .option("header", "true")
    .mode("overwrite")
    .csv(str(CSV_DIR / "full"))
)

print(f"ğŸ“ CSV exportÃ©s dans : {CSV_DIR}")

spark.stop()
print("âœ… votes_enrichis terminÃ©.")
