# organe_usage.py

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count
from pathlib import Path

# -------------------------------------------------
# ğŸ”§ Chemin vers le CSV des organes
# -------------------------------------------------
CSV_PATH = Path("../data_lake/formatted/organes/organes_formatted.csv")

# -------------------------------------------------
# 1ï¸âƒ£ SparkSession
# -------------------------------------------------
spark = (
    SparkSession.builder
    .appName("OrganeUsage")
    .master("local[*]")
    .getOrCreate()
)

print("ğŸ“¥ Lecture du CSV des organes...")
df_raw = (
    spark.read
    .option("header", "true")
    .option("inferSchema", "true")
    .csv(str(CSV_PATH))
)

print("ğŸ“„ SchÃ©ma complet du DataFrame initial :")
df_raw.printSchema()

print("ğŸ” AperÃ§u brut :")
df_raw.show(20, truncate=False)

# -------------------------------------------------
# 2ï¸âƒ£ Colonnes nÃ©cessaires pour le workflow
#     uid = clÃ©
#     codeType = GP ou PARPOL
#     libelle = nom complet
#     libelleAbrev = abrÃ©viation (ex : EPR, LFI-NFP, PCF)
# -------------------------------------------------
colonnes_cible = [
    "uid",
    "codeType",
    "libelle",
    "libelleAbrev",
]

df = df_raw.select(*colonnes_cible).dropDuplicates(["uid"])

print("\nğŸ“„ SchÃ©ma aprÃ¨s sÃ©lection des colonnes usage :")
df.printSchema()

print("\nğŸ” AperÃ§u usage :")
df.show(20, truncate=False)

# -------------------------------------------------
# 3ï¸âƒ£ Quelques stats utiles (optionnel)
# -------------------------------------------------

print("\nğŸ“Š Nombre d'organes par codeType :")
(
    df.groupBy("codeType")
      .agg(count("*").alias("nb"))
      .orderBy(col("nb").desc())
      .show(truncate=False)
)

print("\nğŸ“Š Liste des libellÃ©s uniques par type :")
(
    df.groupBy("codeType", "libelle")
      .count()
      .orderBy("codeType", "libelle")
      .show(50, truncate=False)
)

# -------------------------------------------------
# 4ï¸âƒ£ Sauvegarde en Parquet (layer usage)
# -------------------------------------------------
OUT_PARQUET = Path("../data_lake/usage/organes")
(
    df.write
      .mode("overwrite")
      .parquet(str(OUT_PARQUET))
)

print(f"\nğŸ’¾ DataFrame organes (usage) sauvegardÃ© en parquet dans : {OUT_PARQUET}")

spark.stop()
print("âœ… organe_usage terminÃ©.")
